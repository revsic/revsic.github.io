<!DOCTYPE html>
<html lang="ko-kr"><head>
  <meta charset="utf-8">
  <title>revsic | ML Researcher</title>

  <!-- mobile responsive meta -->
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
  <meta name="description" content="Project Overview">
  <meta name="author" content="YoungJoong Kim">
  <meta name="generator" content="Hugo 0.125.7">

  <!-- plugins -->
  
  <link rel="stylesheet" href="/plugins/bootstrap/bootstrap.min.css ">
  
  <link rel="stylesheet" href="/plugins/slick/slick.css ">
  
  <link rel="stylesheet" href="/plugins/themify-icons/themify-icons.css ">
  
  <link rel="stylesheet" href="/plugins/venobox/venobox.css ">
  

  <!-- Main Stylesheet -->
  
  <link rel="stylesheet" href="/scss/style.min.css" media="screen">

</head><body>
<!-- preloader start -->
<div class="preloader">
  
</div>
<!-- preloader end -->
<!-- navigation -->
<header class="navigation">
  <div class="container">
    
    <nav class="navbar navbar-expand-lg navbar-white bg-transparent border-bottom">
      <button class="navbar-toggler border-0" type="button" data-toggle="collapse" data-target="#navigation">
        <i class="ti-menu h3"></i>
      </button>

      <div class="collapse navbar-collapse text-center" id="navigation">
        <div class="desktop-view">
          <ul class="navbar-nav mr-auto">
            
            <li class="nav-item">
              <a class="nav-link" href="https://github.com/revsic"><i class="ti-github"></i></a>
            </li>
            
            <li class="nav-item">
              <a class="nav-link" href="https://www.linkedin.com/in/young-joong-kim-878630154/"><i class="ti-linkedin"></i></a>
            </li>
            
          </ul>
        </div>

        <ul class="navbar-nav ml-auto">
          <li class="nav-item">
            <a class="nav-link" href="https://revsic.github.io/"> Home </a>
          </li>
          
          
          <li class="nav-item">
            <a class="nav-link" href="/blog">Blog</a>
          </li>
          
          
          
          <li class="nav-item">
            <a class="nav-link" href="/pdf/cv.pdf">CV</a>
          </li>
          
          
        </ul>

        
        <!-- search -->
        <div class="search px-4">
          <button id="searchOpen" class="search-btn"><i class="ti-search"></i></button>
          <div class="search-wrapper">
            <form action="https://revsic.github.io//search" class="h-100">
              <input class="search-box px-4" id="search-query" name="s" type="search" placeholder="Type & Hit Enter...">
            </form>
            <button id="searchClose" class="search-close"><i class="ti-close text-dark"></i></button>
          </div>
        </div>
        

        
      </div>
    </nav>
  </div>
</header>
<!-- /navigation -->

<section class="section-sm">
  <div class="container">
    <div class="row">
      <div class="col-lg-8 mx-auto">
        
        <a href="/categories/portfolio"
          class="text-primary">Portfolio</a>
        
        <h2>Project Overview</h2>
        <div class="mb-3 post-meta">
          <span>By YoungJoong Kim</span>
          
          <span class="border-bottom border-primary px-2 mx-1"></span>
          <span>13 August 2022</span>
          
        </div>
        
        <img src="/images/post/project/head.jpg" class="img-fluid w-100 mb-4" alt="Project Overview">
        
        <div class="content mb-5">
          <p><strong>Cyber Security</strong></p>
<ul>
<li>AIxCC Finalist [GIT:<a href="https://github.com/revsic/aixcc-afc-archive">theori-io/aixcc-afc-archive</a>, <a href="https://theori-io.github.io/aixcc-public/index.html">Blog</a>, <a href="/pdf/Branch_flipper.pdf">Whitepaper</a>], 2024.11.~2025.08. <br>
: <em>미국 고등국방연구계획국 DARPA 주관, 오픈소스 프로젝트 취약점 발굴 자동화 시스템 개발 경연 대회</em></li>
</ul>
<p>R&amp;R: 보안 자동화 연구원, 취약점 발굴 자동화에 관한 연구</p>
<p>Coverage-Guided Greybox Fuzzing은 Random mutation에만 의존할 경우, Branch depth가 깊어짐에 따라 Coverage가 희박해지는 문제를 가집니다. 후속 코드의 탐색을 저해하는 Branch를 특정하여 Fuzz Blocker라 하고, 이를 해소하기 위한 연구를 수행하였습니다.</p>
<p>AIxCC는 취약점 탐색 및 검증, 보완의 전 과정을 전자동화하는 시스템을 개발하는 대회입니다. 주최 측이 제시한 프로젝트에서 자동으로 취약점을 찾고, 보완하는 과정을 점수화하여 순위를 겨룹니다. 저는 <a href="https://theori-io.github.io/aixcc-public/index.html">Team Theori</a>에 속하여, Fuzzing 과정 중 발견한 Fuzz Blocker를 해소하는 자동화 시스템을 담당하였습니다.</p>
<p>미국팀과의 협업하에 연구를 시작한 것은 Fuzz blocker의 탐색입니다. 정적 분석 도구 <a href="https://joern.io/">Joern</a>을 도입하여 Call graph를 확보하고, Background에서 Fuzzer를 구동하며 획득한 Function Coverage를 토대로 Call graph에서 <em>Frontier</em>를 발굴하였습니다. Frontier는 실제 1회 이상 호출된 함수 중, 함수 본문 내 한 번도 호출되지 않은 Callee가 존재하는 경우를 의미합니다.</p>
<p>이후 각 Frontier에 대해 Joern을 활용하여 Branch blocker를 구해내고, 해당 Branch의 위치를 사전에 구축한 LLM Agent에 전달하여 Branch predicate를 풀어내는 입력을 <a href="https://kaitai.io/">Kaitai Struct</a> 형태로 생산합니다. 이 과정에서 GDB Operator Subagent, Code Review Subagent, Query Coverage Subagent를 툴로 두어 Tree 형태의 Agentic System이 운용되도록 하였습니다.</p>
<p>해당 시스템은 실제로 대회에서 활용되었고, 최종 $1.5M 상금의 3위라는 쾌거를 이루었습니다. 자세한 내용은 <a href="/pdf/Branch_flipper.pdf">Whitepaper</a>에서 확인 가능합니다.</p>
<hr>
<ul>
<li>LLM 시스템 위협 분석, 2024.08.~2024.11. <br>
: <em>LLM 시스템 도입 시 발생 가능한 위협 정립, AI 보안 컨설팅 프레임워크 초안 개발</em></li>
</ul>
<p>R&amp;R: AI팀 팀장, 금융권 망 분리 규제 완화 후 LLM 시스템 도입에 관한 AI 보안 컨설팅 사업 기획</p>
<p>24년 중순 금융권의 망 분리 규제가 완화되며, 많은 금융사에서 클라우드 및 AI 서비스의 도입을 고려하기 시작하였습니다. AI 서비스 도입을 위해서는 기업별 보안 전략을 금융보안원에 검수받아야 했고, 관련된 자문 문의가 회사로 이입되기 시작하였습니다.</p>
<p>이후 AI 서비스의 보안 전략에 관한 TF가 신설되었고, AI팀 책임자로 TF에 합류하여 LLM 등 AI 시스템 도입 시 발생 가능한 위협, 그에 따른 실질적 리스크와 대응에 관한 컨설팅 프레임워크의 제작을 본격화하였습니다.</p>
<p>실제 은행권, 금융권에서 Digital/AI Transformation을 수행하는 현황, 전략, 계획을 리서치하여 사례를 정리하고, AI 시스템을 운용해 온 입장에서 경험한 보안 문제, 금융보안원에서 검토 권고 항목으로 제시한 항목을 토대로 공감 가능한 위협과 위험을 정립하였습니다. 이후 실질적 사례를 토대로 예상되는 피해액을 추산, 우선순위를 선정하여 대응 방안을 함께 전달하는 하나의 프레임워크를 구축하고자 하였습니다.</p>
<hr>
<ul>
<li>LLM 기반 위협 시나리오 생성, 2024.01.~2024.08. <br>
: <em>프로젝트 코드가 주어진 환경에서의 위협 발굴 자동화 시스템 개발</em></li>
</ul>
<p>R&amp;R: 보안 자동화 연구원, 취약점 발굴 자동화에 관한 연구</p>
<p>AI팀의 신설 이후, AI를 보안 영역에서 어떻게 활용할 수 있을지 고민해왔습니다.</p>
<p>가장 먼저 시도한 것은 프로젝트 코드가 주어진 상황에서, 발생 가능한 위협을 자동으로 발굴하는 것입니다. 사소한 위협이라도 가능한 모든 위협을 제시할 수 있다면, 실제 보안 감사 담당자가 이를 검수, 보안 문제를 놓치지 않아 서비스 품질을 향상-유지할 수 있을 것이라 기대하였습니다.</p>
<p>다양한 프로그래밍 언어와 서비스를 포괄하는 회사의 보안 감사 서비스 특성상, 특정 언어에 종속된 분석기를 만드는 것 보다는 LLM을 활용하여 포괄적인 분석기를 만드는 것을 목표로 하였습니다. LLM을 통해 함수 단위로 분석 명세를 추출하고, 이를 토대로 객체 단위, 모듈 단위, 종점에는 서비스 수준의 분석 명세를 계층에 맞게 추상화해 나갑니다. 이는 Directed Graph의 형태로 추상화되어 정보의 입출력을 Edge로 표현하고, 이를 토대로 공격 벡터가 정리됩니다.</p>
<p>이후 LLM은 각 추상 계층의 Local Subgraph를 입력으로 받아 가능한 위협을 모두 정리하고, 중복된 위협이 제거된 최종 위협 보고서가 출력됩니다.</p>
<p>실제로 Curl, aiohttp, Mastodon에서 발생하였던 1-day 중 5건이 이를 토대로 발견 가능함을 확인하였지만, False positive가 많아 실질적 활용은 쉽지 않은 상황이었습니다. 이는 이후 AIxCC의 Fuzzing 기반 Exploitability Validator System을 개발하는 방식으로 대응되었습니다.</p>
<hr>
<p><strong>Vision</strong></p>
<ul>
<li>Stable Diffusion 기반 선화 채색, 2023.04.~2023.07. <br>
: <em>웹툰 선화 자동 채색 어시스턴트 개발</em></li>
</ul>
<p>R&amp;R: 영상 합성 연구원, 합성 이미지의 일관성에 관한 연구</p>
<p>웹툰 시장에 대한 비즈니스 수요를 확인한 이후, 웹툰 제작 어시스턴트 개발을 태핑 하였습니다. 가장 먼저 점검한 기술은 선화의 1차 채색 단계를 반자동화하는 것이었습니다.</p>
<p>Stable Diffusion(이하 SD)에는 LineArt ControlNet[<a href="https://huggingface.co/lllyasviel/ControlNet-v1-1">HF</a>]이 있어, 선화를 조건으로 합성된 이미지가 선화를 유지하며, 빈 영역을 채색한 이미지를 제공합니다. 하지만 채색된 이미지에 비현실적 음영이 들어가 있거나, 이미지마다 의상의 색상이 다른 등 일관성에 문제를 가지고 있었습니다.</p>
<p>또한 SD 위에서 LoRA를 통해 웹툰 캐릭터를 학습하여도, ControlNet이 도입되고 나면 색상 일관성이 유지되지 않는 등 Perturbation에 유약한 모습을 보입니다.</p>
<p>이를 해결하기 위하여 학습 파이프라인에 ControlNet 입력을 함께 두어 LoRA가 ControlNet에 의한 Perturbation 위에서 색상을 학습할 수 있게 두었고, SD의 결과물을 그대로 사용하는 것이 아닌 선화용 Segmentation 모듈과 각 세그먼트에 대한 Repainter 모듈 등을 개발하여 후처리에 사용-부족한 일관성을 개선하였습니다.</p>
<hr>
<ul>
<li>Stable Diffusion 기반 Image-to-Video Style Transfer, 2023.03. <br>
: <em>이미지 생성 모델 기반 영상 합성 확장</em></li>
</ul>
<p>R&amp;R: 1인 연구, 영상 합성 연구원, 합성 이미지의 일관성, 연속성에 관한 연구</p>
<p>영상 합성 모델을 내재화하는 과정에서 <a href="https://research.runwayml.com/gen2">Runway/Gen-2</a>의 &ldquo;04.Stylization&rdquo; 데모를 인상 깊게 보아, 사내에서도 이미지의 스타일을 비디오에 Transfer할 수 있는지 PoC를 수행하였습니다.</p>
<p>Video Translation에서 SD를 사용하기 위해서는 비디오의 각 프레임을 Image-to-Image Translation해야 합니다.
하지만 Stable Diffusion(이하 SD)은 합성되는 이미지의 높은 분산으로 인해 일관되고 연속된 이미지를 생성하기 어렵습니다. (Video Diffusion 모델은 사내 GPU 리소스로는 운용이 어려웠습니다.)</p>
<p>첫 번째 프레임을 SD와 ControlNet(Depth, Skeleton, Colorization)을 기반으로 I2I Transfer 하여 그럴듯한 이미지를 확보하더라도, 두 번째 프레임 합성 결과가 첫 번째와 연속적으로 이어지지 않습니다.</p>
<p>DDIM Latent Inversion[<a href="https://arxiv.org/abs/2010.02502">arXiv:2010.02502</a>], Edit-friendly inversion[<a href="https://arxiv.org/abs/2304.06140">arXiv:2304.06140</a>], Null-text inversion[<a href="https://arxiv.org/abs/2211.09794">arXiv:2211.09794</a>] 등 당시 inversion 기법은 latent-spatial correlation의 한계를 가지고 있었습니다. 이상적으로는 영상 내 객체의 움직임에 따라 style pattern이 함께 이동해야 하지만, inverted latent가 spatial bias를 가져 style pattern의 위치는 이미지 내에 고정되고, 영상 내 객체가 패턴 위를 지나가는 듯한 부자연스러운 artifact를 발생시켰습니다.</p>
<p>이를 완화하기 위해 MasaCtrl[<a href="https://arxiv.org/abs/2304.08465">arXiv:2304.08465</a>] 등 방법론에 영감을 받아 ControlNet 사용을 가정한 상태에서 SD의 Self-attention을 Inter-frame Cross-attention layer로 교체하고, latent variable을 영상의 움직임에 따라 warping하여 spatial correlation을 자연스럽게 재구성하는 등의 작업을 수행하였습니다.</p>
<hr>
<ul>
<li>Unique Image Synthesis, 2022.06. <br>
: <em>합성 이미지의 다양화에 관한 연구</em></li>
</ul>
<p>R&amp;R: 영상 합성 연구원, 생성 모델의 분산에 관한 연구</p>
<p>생성 모델은 대개 유한한 분산을 가집니다. 특히나 적은 데이터로 Finetuning한 모델이 생성하는 이미지의 분산은 경험적으로 기존의 것보다 작습니다. 중복을 허용하지 않을 때, 모델이 생성할 수 있는 최대한의 이미지를 합성하기에 random sampling은 충분히 효율적인 방법론이 아닙니다.</p>
<p>제품에 출시할 가상 인간을 최대한 많이, 다양하게 만들기 위해, 잠재 변수 공간을 탐색하는 알고리즘을 구축하였습니다.</p>
<p>가장 먼저 이미지의 유사도를 정의하고, threshold를 설정하였습니다. latent space를 분석하여 유사도와 정렬된 축을 찾고, 해당 축을 기준으로 공간 탐색을 시도하였습니다. 탐색을 진행할 때는 이전까지 합성되었던 이미지들과 유사도를 비교하여 임계점을 넘는 이미지만을 데이터베이스에 추가하였습니다.</p>
<p>기존 random sampling에 비해 7~10배 빠르게 이미지를 합성하는 속도 개선을 확인하였습니다.</p>
<hr>
<p><strong>Speech</strong></p>
<ul>
<li>NANSY++, 2022.10. ~ 2023.01. <br>
: <em>음성 변조를 위한 목소리 분석/재합성 모델 구축</em></li>
</ul>
<p>R&amp;R: 2인 연구, 음성 합성 연구원, 음성 변조 모델 개발을 위한 연구 개발</p>
<p>음성 변조(Voice Conversion, 이하 VC) 모델은 언어적 신호(Linguistic Signal)와 화자의 발화 습관(음색, 높낮이, 크기, 발음 습관, etc.)을 분석하여 재합성하는 방식으로 이뤄집니다.</p>
<p>NANSY(Neural Analysis and Synthesis, <a href="https://arxiv.org/abs/2110.14513">arXiv:2110.14513</a>) 는 Supertone에서 개발한 모델로, 음성으로부터 언어 신호와 화자의 특성을 분리한 후, 재합성하는 음성 모델입니다. 이를 통해 음성의 높낮이를 바꾸거나, 목소리를 바꾸는 등(VC)의 변조가 가능합니다.</p>
<p>음성 변조에 대한 비즈니스 수요가 확인된 이후, 당시 SOTA VC 모델인 NANSY와 후속 연구인 NANSY++[<a href="https://arxiv.org/abs/2211.09407">arXiv:2211.09407</a>]을 재현하는 업무를 수행하였습니다.</p>
<p>공개된 구현체가 없었기에, Yingram, CQT 기반 Pitch Tracker, Sinusoidal Signal Generator 등의 기반 모델을 구현하고, NANSY를 최종 학습 시도하였습니다.</p>
<ul>
<li>NANSY: <a href="https://github.com/revsic/torch-nansy">https://github.com/revsic/torch-nansy</a></li>
<li>NANSY++: <a href="https://github.com/revsic/torch-nansypp">https://github.com/revsic/torch-nansypp</a></li>
</ul>
<hr>
<ul>
<li>Stable TTS, 2021.09. ~ 2021.12. <br>
: <em>TTS 합성 실패 방지의 이론적 해결책에 관한 연구</em></li>
</ul>
<p>R&amp;R: 1인 연구, 음성 합성 연구원, 합성 실패 방지를 위한 연구 수행</p>
<p>근래의 대부분 딥러닝 모델은 BatchNorm이나 InstanceNorm을 활용합니다. 이 중 BatchNorm은 학습 과정에서 추정한 이동 통계량을 기반으로 표준화를 진행합니다. 만약 학습에 활용한 데이터의 양이 충분하지 않아 통계치가 일반화되지 않았다면 miss-normalization 문제가 발생할 수 있습니다.</p>
<p>저량의 데이터로 학습된 합성 모델에서 음성이 오합성 되는 이슈가 있었고, 분석 결과 BatchNorm의 miss-normalization에 의한 feature map의 variance exploding 현상을 원인으로 확인하였습니다.</p>
<p>이를 해결하기 위해 RescaleNet[<a href="https://papers.nips.cc/paper/2020/file/9b8619251a19057cff70779273e95aa6-Paper.pdf">NeurIPS2020</a>], LayerScale[<a href="https://arxiv.org/abs/2103.17239v2">arXiv:2103.17239</a>], InstanceNorm 등으로 대체하는 연구를 진행하였습니다.</p>
<hr>
<ul>
<li>Latent system, 2021.04. ~ 2021.08. <br>
: <em>non-parallel 데이터와 unseen property의 일반화 가능성에 관한 연구</em></li>
</ul>
<p>R&amp;R: 1인 연구, 음성 합성 연구원, 다국어 모델 개발을 위한 연구 수행</p>
<p>음성은 크게 발화자/언어/비언어 표현 3가지 관점에서 관찰할 수 있습니다. 이중 각 도메인의 클래스 간 모든 조합을 데이터로 구성하는 것을 parallel data, 일부 케이스가 비는 것을 non-parallel data라고 할 때, non-parallel 환경에서 문장 내 화자와 언어 정보를 분리하는 것은 natural하게 이뤄질 수 없습니다.</p>
<ul>
<li>ex. [인물A/B, 영어/한글], parallel: 인물A/한글, 인물A/영어, 인물B/한글, 인물B/영어</li>
<li>natural: 케이스가 비는 경우, 별도의 장치 없이 화자와 언어를 조건화하는 것만으로는 unseen pair의 합성 품질을 보장할 수 없습니다.</li>
</ul>
<p>따라서 non-parallel 환경에서 다화자-다국어 음성 합성 모델을 개발하는 경우, 특정 화자에서 관측되지 않은 언어 정보, unseen property에 대한 일반화가 이뤄질 수 있어야 합니다.</p>
<p>Latent System 연구에서는 VAE와 GAN 등 방법론을 통해 Latent variable을 도입하고, 정보의 흐름을 보다 명확히 관리하는 것을 목표로 합니다. CLUB[<a href="https://arxiv.org/abs/2006.12013">arXiv:2006.12013</a>]을 활용한 국소-전역부의 잠재 변수 분리, CycleGAN[<a href="https://arxiv.org/abs/1703.10593">arXiv:1703.10593</a>]을 활용한 unseen-property 일반화 등을 가설로 연구를 수행하였습니다.</p>
<p>다음은 당시 모델로 만들었던 프로토타입 영상입니다.</p>
<ul>
<li><a href="https://www.youtube.com/watch?v=38LrO_cbAyU">youtube:Lionrocket</a></li>
</ul>
<hr>
<ul>
<li>Semi-Autoregressive TTS, 2020.12. ~ 2021.04. <br>
: <em>합성 속도와 음질상 이점의 Trade-off에 관한 연구</em></li>
</ul>
<p>R&amp;R: 음성 합성 연구원, 베이스라인 개선 실험 수행</p>
<p>TTS 모델은 Autoregressive(이하 AR) 모델과 Duration 기반의 Parallel(이하 PAR) 모델로 나뉩니다. AR 모델은 대체로 합성 속도가 음성의 길이에 비례하여 느려지지만 전반적인 음질 수준이 높고, PAR 모델은 상수 시간에 가까운 합성 속도를 가지지만 전반적으로 노이즈 수준이 높은 편입니다.</p>
<p>Semi-Autoregressive TTS 연구는 이 둘을 보완하기 위한 연구입니다. AR TTS의 병목은 대부분은 AR 방식의 Alignment에서 오기에, Alignment는 Duration 기반의 PAR 모델을 따르고, 이후 Spectrogram 생성은 Autoregression하는 방식의 가설로 삼았습니다. 이는 DurIAN[<a href="https://arxiv.org/abs/1909.01700">arXiv:1909.01700</a>], NAT[<a href="https://arxiv.org/abs/2010.04301">2010.04301</a>]와 유사합니다.</p>
<p>이후 추가 개선을 거쳐 실시간에 가까운 AR 모델을 개발하였지만, 음질의 중요성이 높아지며 추가 개선 및 배포가 보류된 프로젝트입니다.</p>
<hr>
<ul>
<li>TTS Baseline, 2019.09. ~ 2020.10. <br>
: <em>Text-to-Speech 음성 합성 모델 베이스라인 선정에 관한 연구</em></li>
</ul>
<p>R&amp;R: 음성 합성 연구원, 오픈소스 검토, 논문 구현, Ablation</p>
<p>TTS 모델의 베이스라인 선정에 관한 연구입니다. Autoregressive 모델인 Tacotron[<a href="https://arxiv.org/abs/1703.10135">arXiv:1703.10135</a>]부터 Duration 기반의 parallel 모델인 FastSpeech2[<a href="https://arxiv.org/abs/2006.04558">arXiv:2006.04558</a>] 등을 폭넓게 검토하였습니다. 검토 과정에서 어떤 백본을 썼을 때 발음이나 음질 오류가 줄어드는지 검토하고, Duration을 어떤 모델을 통해 추정할지, Joint training이 가능한지를 연구하였습니다.</p>
<p>Acoustic 모델이 완료된 후에는 Vocoder 군에서 Autoregressive 모델인 WaveNet[<a href="https://arxiv.org/abs/1609.03499">arXiv:1609.03499</a>], WaveRNN[<a href="https://arxiv.org/abs/1802.08435">arXiv:1802.08435</a>] LPCNet[<a href="https://arxiv.org/abs/1810.11846">arXiv:1810.11846</a>]과 Parallel 모델인 MelGAN[<a href="https://arxiv.org/abs/1910.06711">arXiv:1910.16711</a>] 등을 검토하였습니다. 이후 LPCNet에서 영감을 받아 Source-filter 기반의 방법론을 GAN 기반의 Parallel 모델에 적용하여 음질 개선이 이뤄질 수 있는지 연구하였습니다.</p>
<p>연구된 베이스라인은 TTS 서비스인 <a href="https://onairstudio.ai/">On-air studio</a>에서 활용하고 있습니다.</p>
<details>
    <summary>다음은 그 외 사이드 프로젝트로 구현한 TTS 모델입니다.</summary>
    <ul>
<li>
<p>torch-retriever-vc [<a href="https://github.com/revsic/torch-retriever-vc">GIT</a>], 2023.01. <br>
: *Retriever: Learning Content-Style Representation as a Token-Level Bipartite Graph, Yin et al., 2022.</p>
</li>
<li>
<p>torch-diffusion-wavegan [<a href="https://github.com/revsic/torch-diffusion-wavegan">GIT</a>], 2022.03. <br>
: <em>Parallel waveform generation with DiffusionGAN, Xiao et al., 2021.</em></p>
</li>
<li>
<p>torch-tacotron [<a href="https://github.com/revsic/torch-tacotron">GIT</a>], 2022.02. <br>
: <em>PyTorch implementation of Tacotron, Wang et al., 2017.</em></p>
</li>
<li>
<p>tf-mlptts [<a href="https://github.com/revsic/tf-mlptts">GIT</a>], 2021.09. <br>
: <em>Tensorflow implementation of MLP-Mixer based TTS.</em></p>
</li>
<li>
<p>jax-variational-diffwave [<a href="https://github.com/revsic/jax-variational-diffwave">GIT</a>], [<a href="https://arxiv.org/abs/2107.00630">arXiv:2107.00630</a>], 2021.09. <br>
: <em>Variational Diffusion Models</em></p>
</li>
<li>
<p>tf-glow-tts [<a href="https://github.com/revsic/tf-glow-tts">GIT</a>] [<a href="https://arxiv.org/abs/2005.11129">arXiv:2005.11129</a>], 2021.07. <br>
: <em>Glow-TTS: A Generative Flow for Text-to-Speech via Monotonic Alignment Search</em></p>
</li>
<li>
<p>tf-diffwave [<a href="https://github.com/revsic/tf-diffwave">GIT</a>] [<a href="https://arxiv.org/abs/2009.09761">arXiv:2009.09761</a>], 2020.10. <br>
: <em>DiffWave: A Versatile Diffusion Model for Audio Synthesis, Zhifeng Kong et al., 2020.</em></p>
</li>
</ul>

</details>
<hr>
<p><strong>Engineering</strong></p>
<ul>
<li>face_provider [GIT:<a href="https://github.com/lionrocket-inc/">lionrocket-inc</a>/private], 2022.06 <br>
: <em>All-in-one Face generation API</em></li>
</ul>
<p>얼굴 인식, 검색, 합성, 분류, 추천 목적 통합 서비스 지원 프레임워크 <br>
Skills: Python, PyTorch, dlib, opencv, FAISS <br>
R&amp;R: 1인 개발</p>
<p>통합 얼굴 이미지 지원 프레임워크입니다. 이미지 내 얼굴 탐지를 시작으로 정렬, 인식, 분류, 벡터 데이터베이스에서의 검색과 추천을 지원합니다.</p>
<p>얼굴 탐지와 인식 과정에는 입력 이미지의 회전량에 따라 인식 성능이 떨어지는 문제가 있었고, 이를 보정하기 위해 두상의 회전량을 추정하여 이미지를 정면으로 정렬하거나, 인식이 불가능한 이미지를 사전에 고지할 수 있게 구성하였습니다.</p>
<p>이후 검색과 분류, 추천 과정이 실시간으로 이뤄져야 한다는 기획팀의 요청이 있었고, 벡터 검색 과정은 MetaAI의 벡터 검색 시스템 <a href="https://github.com/facebookresearch/faiss">FAISS</a>를 활용하여 최적화를 진행하였습니다. 얼굴형에 관한 초기 분류 모델은 <a href="http://dlib.net/">dlib</a>의 Facial Landmark를 기반으로 작동하였으나, <a href="http://dlib.net/">dlib</a>은 실시간 구성이 어렵다는 문제가 있었고, 추후 <a href="https://google.github.io/mediapipe/">Mediapipe</a> 교체를 고려하고 있습니다.</p>
<hr>
<ul>
<li>CULICULI [GIT:<a href="https://github.com/lionrocket-inc/">lionrocket-inc</a>/private], 2020.07.10 <br>
: <em>CUDA Lib for LionRocket</em></li>
</ul>
<p>C++ CUDA Native를 활용하여 딥러닝 추론 속도를 10배 가량 가속화한 프레임워크 <br>
Skills: C++, CUDA, Python, PyBind <br>
R&amp;R: 1인 개발</p>
<p>음성 합성 파이프라인의 추론 가속화를 위해 C++ CUDA Native를 활용하여 10배가량 합성 시간을 단축시킨 프로젝트입니다. C++과 CUDA를 통해 기본적인 Tensor 객체와 BLAS(Basic Linear Algebra Subroutines)를 구성하고, 합성 속도를 최적화한 후, <a href="https://pybind11.readthedocs.io/en/stable/">PyBind</a>를 통해 python 인터페이스를 제공하였습니다.</p>
<p>당시 TTS 모델에는 음성의 길이에 합성 시간이 비례하는 문제가 있었고, 단위 시간을 줄여 거의 실시간에 가까운 합성 속도를 구성할 수 있어야 했습니다. 이를 위해 C++로 BLOB-Shape Tuple 형태의 Tensor 객체를 구축하고, 템플릿 프로그래밍을 통해 이를 CUDA Native에서도 활용할 수 있게 두었습니다.</p>
<p>BLAS 구현과 POC 이후 병목이 메모리 할당에 있음을 확인하여, 메모리 풀과 CUDA API를 활용하지 않는 자체적인 메모리 할당 방식을 구성, 대략 5~7배의 속도 향상을 확인할 수 있었습니다.</p>
<p>이렇게 만들어진 프레임워크를 팀에서 활용하고자 했고, LR_TTS에서 학습된 체크포인트를 파이썬 인터페이스로 실행 가능하도록 <a href="https://pybind11.readthedocs.io/en/stable/">PyBind</a>를 활용하였습니다.</p>
<hr>
<ul>
<li>LR_TTS [GIT:<a href="https://github.com/lionrocket-inc/">lionrocket-inc</a>/private], 2019.09 <br>
: <em>PyTorch implementation of TTS base modules</em></li>
</ul>
<p>음성 데이터 전처리, 모델 구현, 학습, 데모, 패키징, 배포까지의 파이프라인을 구성한 프레임워크 <br>
Skills: Python, PyTorch, Librosa, Streamlit, Tensorboard <br>
R&amp;R: 기획, 개발, 배포, 총책임</p>
<p>음성 합성팀의 통합 연구 환경을 위한 플랫폼 개발 프로젝트입니다. 당시 PyTorch에는 Keras나 Lightning과 같이 단순화된 프레임워크가 부재했기에 데이터 생성부터 연구, 개발, 학습, 패키징, 평가, 배포, 데모 등 일련의 과정을 프로세스화 하고 코드 재사용성을 극대화하여 적은 리소스로 연구자가 부담없이 배포가 가능하도록 구성했습니다.</p>
<p>자사 내의 데이터 전처리 구조를 단순화하고, 모든 학습이 고정된 프로토콜 내에서 가능하도록 모델 구조와 콜백 함수를 추상화하여 연구 프로세스를 정리했습니다. 또한 패키징과 배포의 단순화를 위해 모델 구조와 하이퍼파라미터를 분리, 각각을 고정된 프로토콜에 따라 저장, 로딩하는 모든 과정이 자동화될 수 있도록 구성했습니다.</p>
<p>개발 중 UnitTest와 CI를 도입해보았지만, 딥러닝 모델의 테스트 방법론이 일반적인 소프트웨어 테스트 방법론과는 상이한 부분이 존재했고, 끝내 테스트가 관리되지 않아 현재는 테스트를 제거한 상태입니다.</p>
<p>CI의 경우에는 이후 PR 생성에 따라 자동으로 LR_TTS의 버전 정보를 생성하고, on-premise framework에 모델을 자동으로 배포할 수 있도록 구성하였습니다.</p>
<hr>
<ul>
<li>Behavior based Malware Detection Using Branch Data [<a href="https://github.com/revsic/tf-branch-malware">GIT</a>], 2019.08. <br>
: <em>Classify malware from benign software using branch data via LSTM based on Tensorflow</em></li>
</ul>
<p>브랜치 데이터를 통한 행위 기반 멀웨어 탐지 기법 연구 <br>
Skills: C++, Windows Internal, PE, Cuckoo Sandbox, Python, Tensorflow <br>
R&amp;R: 1인 연구</p>
<p><a href="https://docs.microsoft.com/en-us//windows/win32/debug/vectored-exception-handling">VEH</a>를 기반으로 분기구문(branch instruction)을 추적하는 <a href="https://github.com/revsic/BranchTracer">Branch Tacer</a>를 구현한 후, DLL Injection 방식을 통해 보안 가상 환경(sandbox)에서 멀웨어와 일반 소프트웨어의 분기 정보(branch data)를 축적, <a href="https://github.com/revsic/tf-branch-malware">딥러닝 기반 탐지 모델</a>을 개발하였습니다.</p>
<p>Sandbox 환경 내에서는 MSR을 사용할 수 없어 VEH를 통해 branch tracer를 직접 구현해야 했고, 분기문 탐색을 위해 디스어셈블러의 일부를 직접 구현하면서 기술적 어려움을 겪었습니다. 이는 후에 인텔 매뉴얼을 참고하며 tracer를 완성하였고, 이후 이를 발전시켜 VEH 기반의 DBI(Dynamic Binary Instrumentation)[<a href="https://github.com/revsic/cpp-veh-dbi">GIT:cpp-veh-dbi</a>] 도구를 구현할 수 있었습니다.</p>
<p>딥러닝 모델은 LSTM 기반의 간단한 시퀸스 모델을 이용하였고, 결과 88% 정도의 정확도를 확인할 수 있었습니다.</p>
<p>이는 당시 논문의 형태로 정리되어 <a href="https://www.kiise.or.kr/">정보과학회</a> 2017년 한국컴퓨터종합학술대회 논문집[<a href="https://www.dbpia.co.kr/journal/articleDetail?nodeId=NODE07207863">PAPER</a>]에 고등학생 부문으로 기재되었습니다.</p>

        </div>

        
        
      </div>
    </div>
  </div>
</section>

<footer>
  <div class="container">
    <div class="row">
      <div class="col-12 text-center mb-5">
      </div>
      <div class="col-lg-3 col-sm-6 mb-5">
        <h6 class="mb-4">Contact Me</h6>
        <ul class="list-unstyled">
          
          <li class="mb-3"><i class="ti-location-pin mr-3 text-primary"></i>Seoul, Korea</li>
          <li class="mb-3"><a class="text-dark" href="mailto:revsic99@gmail.com"><i
                class="ti-email mr-3 text-primary"></i>revsic99@gmail.com</a>
          </li>
        </ul>
      </div>
      <div class="col-lg-3 col-sm-6 mb-5">
        <h6 class="mb-4">Social Contacts</h6>
        <ul class="list-unstyled">
          
          <li class="mb-3"><a class="text-dark" href="https://github.com/revsic">github</a></li>
          
          <li class="mb-3"><a class="text-dark" href="https://www.linkedin.com/in/young-joong-kim-878630154/">linkedin</a></li>
          
        </ul>
      </div>
      <div class="col-lg-3 col-sm-6 mb-5">
        <h6 class="mb-4">Categories</h6>
        <ul class="list-unstyled">
          <li class="mb-3"><a class="text-dark"
              href="/categories/attention">Attention</a>
          </li>
          <li class="mb-3"><a class="text-dark"
              href="/categories/bayesian">Bayesian</a>
          </li>
          <li class="mb-3"><a class="text-dark"
              href="/categories/generative">Generative</a>
          </li>
          <li class="mb-3"><a class="text-dark"
              href="/categories/portfolio">Portfolio</a>
          </li>
          <li class="mb-3"><a class="text-dark"
              href="/categories/software-testing">Software testing</a>
          </li>
          <li class="mb-3"><a class="text-dark"
              href="/categories/vocoder">Vocoder</a>
          </li>
          <li class="mb-3"><a class="text-dark"
              href="/categories/writing">Writing</a>
          </li>
        </ul>
      </div>
      <div class="col-lg-3 col-sm-6 mb-5">
        <h6 class="mb-4">Quick Links</h6>
        <ul class="list-unstyled">
          
          <li class="mb-3"><a class="text-dark" href="/">Home</a></li>
          
          <li class="mb-3"><a class="text-dark" href="/blog">Blog</a></li>
          
          <li class="mb-3"><a class="text-dark" href="/pdf/cv.pdf">CV</a></li>
          
        </ul>
      </div>
      <div class="col-12 border-top py-4 text-center">
        | copyright © 2020 <a href="https://revsic.github.io">YoungJoong Kim</a> All Rights Reserved |
      </div>
    </div>
  </div>
</footer>

<script>
  var indexURL = "/index.json"
</script>

<!-- JS Plugins -->

<script src="/plugins/jQuery/jquery.min.js"></script>

<script src="/plugins/bootstrap/bootstrap.min.js"></script>

<script src="/plugins/slick/slick.min.js"></script>

<script src="/plugins/venobox/venobox.min.js"></script>

<script src="/plugins/search/fuse.min.js"></script>

<script src="/plugins/search/mark.js"></script>

<script src="/plugins/search/search.js"></script>

<!-- Main Script -->

<script src="/js/script.min.js"></script>

<script>
  MathJax = {
    tex: {
      inlineMath: [['$', '$']],
    }
  }
</script>
<script async src="https://cdn.jsdelivr.net/npm/mathjax@4.0.0-beta.4/tex-mml-chtml.js"></script>
</body>
</html>